<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Ordinary least squares regression | Matthew Stone</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Ordinary least squares regression" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Course logistics and overview" />
<meta property="og:description" content="Course logistics and overview" />
<link rel="canonical" href="http://localhost:4002/notes/ece532/lecture-01" />
<meta property="og:url" content="http://localhost:4002/notes/ece532/lecture-01" />
<meta property="og:site_name" content="Matthew Stone" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-22T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Course logistics and overview","@type":"BlogPosting","url":"http://localhost:4002/notes/ece532/lecture-01","headline":"Ordinary least squares regression","dateModified":"2019-01-22T00:00:00-06:00","datePublished":"2019-01-22T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4002/notes/ece532/lecture-01"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4002/feed.xml" title="Matthew Stone" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Matthew Stone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/notes/">Lecture notes</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Ordinary least squares regression</h1>
    <p class="post-meta">
    , Lecture 1 &mdash;
      <time class="dt-published" datetime="2019-01-22T00:00:00-06:00" itemprop="datePublished">Jan 22, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="course-logistics-and-overview">Course logistics and overview</h1>

<ul>
  <li>Professor Po-Ling Loh has office hours in MSC 1212 Tuesdays after class.
Muni, the TA, has office hours Fridays 3:30-5:30.</li>
  <li>Homeworks are assigned biweekly and due Tuesdays at 2:30pm via PDF submission
to gradescope.</li>
  <li>Expected prereqs are linear algebra, probability, stats, and familiarity with
matlab or another matrix computing environment (e.g. Python, Julia, R)</li>
  <li>Grading: midterm 1 (25%), midterm 2 (25%), final (30%), homeworks (20%;
lowest dropped)</li>
  <li>Texts: Bishop, Elden recommended, Hastie suggested</li>
  <li>Topics:
    <ol>
      <li>Regression</li>
      <li>Dimensionality reduction</li>
      <li>Clustering</li>
      <li>Other (as time permits) - neural networks, tensor methods</li>
    </ol>
  </li>
</ul>

<p>Today in class we showed three different but equivalent derivations of the
ordinary least squares estimate for linear regression.</p>

<script type="math/tex; mode=display">%% Latex helpers
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\R}{\mathbb{R}}</script>

<h1 id="linear-regression-1-dimensional">Linear regression (1-dimensional)</h1>

<p>Goal: given data pairs <script type="math/tex">\{(x_i, y_i)\}_{i=1}^{n}</script>, where <script type="math/tex">x_i, y_i \in \R</script>, find the best fit line
<script type="math/tex">y = ax + b</script> through data.</p>

<p>The least squares solution to this problem is:</p>

<script type="math/tex; mode=display">\min\limits_{a, b} \sum\limits_{i=1}^n (y_i - (a x_i + b))^2</script>

<p>and our objective function is the sum of squared distances:</p>

<script type="math/tex; mode=display">f(a, b) = \sum (y_i - a x_i + b))^2.</script>

<p>Traditionally, to solve this optimization problem, we take the partial
derivative of <script type="math/tex">f</script> with respect to each variable, set each partial derivative
equal to 0, and solve the corresponding system of equations.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\frac{\partial f}{\partial a} &= \sum -2 x_i (y_i - a x_i - b) = 0 \\
\frac{\partial f}{\partial b} &= \sum -2 (y_i - a x_i - b) = 0
\end{align*} %]]></script>

<p>To solve for <script type="math/tex">a</script> and <script type="math/tex">b</script>, construct the system of equations:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
a \sum x_{i}^2 + b \sum x_i &= \sum x_i y_i \\
a \sum x_i + b n &= \sum y_i
\end{align*} %]]></script>

<p>Multiply the upper equation by <script type="math/tex">n</script> and the lower equation by <script type="math/tex">\sum x_i</script>, and subtract to eliminate the <script type="math/tex">b</script> term, then solve for <script type="math/tex">a</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
a (n \sum x_{i}^2 - (\sum x_i)(\sum x_i)) &= n \sum x_i y_i - \sum y_i \sum x_i \\
a &= \frac{n \sum x_i y_i - \sum y_i \sum x_i}{n \sum x_{i}^2 - (\sum x_i)(\sum x_i)}
\end{align*} %]]></script>

<p>and, by the lower equation above, we have</p>

<script type="math/tex; mode=display">b = \frac{-a \sum x_i + \sum y_i}{n}.</script>

<p>The point we’d like to make in the next section is that we can skip the laborious calculus and algebra with matrix methods.</p>

<p>So, how do we generalize to more than one dimension?</p>

<h1 id="linear-regression-general-case">Linear regression (general case)</h1>

<p>Now let us consider the case where <script type="math/tex">y_i \in \mathbb{R}</script>, but <script type="math/tex">x_i \in
\mathbb{R}^p</script> is higher dimensional. (For example, let us consider using an
individual’s height and weight to predict body mass index (BMI)). We would like
to obtain the best linear predictor of <script type="math/tex">y_i</script> using all components of <script type="math/tex">x_i</script>.</p>

<p>That is,</p>

<script type="math/tex; mode=display">y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} = \beta'x_i</script>

<p>for some <script type="math/tex">\beta \in \mathbb{R}^p</script>. (Note that for the remainder of this
discussion, we are considering <script type="math/tex">x_{i1}</script> to be <script type="math/tex">1</script> and <script type="math/tex">\beta_1</script> to be the
intercept term.)</p>

<p>So now let us reformulate the least squares solution to our regression problem:</p>

<script type="math/tex; mode=display">\min\limits_{\beta_1, \ldots, \beta_p} \sum\limits_{i=1}^{n} (y_i - x_i' \beta)^2</script>

<p>and our new objective function is</p>

<script type="math/tex; mode=display">f(\beta) = \sum (y_i - x_i' \beta)^2.</script>

<p>The naive approach we could employ from the previous section would be to take
the partial derivative of <script type="math/tex">f</script> with respect to each <script type="math/tex">\beta_j</script>, and solve the
system of equations. Let us instead take the gradient of <script type="math/tex">f(\beta)</script> with
respect to <script type="math/tex">\beta</script> and set it equal to 0.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\beta} f(\beta) &= \nabla_{\beta} \left(\sum(y_i - x_i'\beta)^2\right) \\
                        &= \sum \nabla_{\beta} [(y_i - x_i'\beta)^2] && \text{Derivation is linear} \\
                        &= \sum 2 (y_i - x_i' \beta) \nabla_{\beta}(y_i - x_i' \beta)  && \text{Chain rule} \\
                        &= \sum -2(y_i -x_i'\beta) \nabla_{\beta}(x_i'\beta) && \nabla_{\beta}y_i = 0 \\
                        &= \sum -2(y_i - x_i' \beta) x_i
\end{align*} %]]></script>

<p>For the final step of the above calculation, note that <script type="math/tex">\nabla_{\beta}
x_i'\beta = x_i</script> . To show this, observe that the <script type="math/tex">k</script>-th entry of the
gradient of <script type="math/tex">x_i'\beta</script>, is the partial derivative with respect <script type="math/tex">\beta_k</script>.</p>

<script type="math/tex; mode=display">(\nabla_{\beta} (x_i' \beta))_k = \frac{\partial}{\partial \beta_k} x_i' \beta = \frac{\partial}{\partial \beta_k} \sum\limits_{j=1}^{p} x_{ij} \beta_j = x_{ik}</script>

<p>So we have</p>

<script type="math/tex; mode=display">\nabla_{\beta} (x_i' \beta) = \begin{pmatrix} x_{i1} \\ x_{i2} \\ \vdots \\ x_{ip} \end{pmatrix} = x_i.</script>

<h4 id="sanity-checking-our-math">Sanity checking our math</h4>

<p>We pause here to highlight the importance of sanity checking our calculus and
algebra by checking that the dimensions match across variables.</p>

<script type="math/tex; mode=display">\nabla f(\beta) = \sum -2 (y_i - x_i' \beta) x_i = 0</script>

<p>Here, <script type="math/tex">y_i</script> is 1-dimensional. <script type="math/tex">x_i'</script> is <script type="math/tex">1 \times p</script> and <script type="math/tex">\beta</script> is
<script type="math/tex">p</script>-dimensional, so <script type="math/tex">x_i' \beta</script> is a scalar, and therefore the
parenthetical quantity is a scalar. We can multiply <script type="math/tex">x_i</script>, a
<script type="math/tex">p</script>-dimensional column vector, by this scalar, and we are thus setting the
equation equal to the <script type="math/tex">p</script>-dimensional zero vector. This is consistent with
our knowledge that the gradient is a vector-valued function in
<script type="math/tex">\mathbb{R}^p</script>.</p>

<h4 id="solving-the-gradient-for-beta">Solving the gradient for <script type="math/tex">\beta</script></h4>

<p>Now that we have a tractable formulation of our gradient, we can manipulate it
in a smart way to solve for <script type="math/tex">\beta</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\sum\limits_{i} (y_i - x_i' \beta) x_i &= 0 \\
\sum\limits_{i} y_i x_i - \sum\limits_{i} x_i' \beta x_i &= 0 \\
\sum\limits_{i} (x_i' \beta) x_i &= \sum\limits_{i} y_i x_i \\
\sum\limits_{i} x_i (x_i' \beta) &= \sum\limits_{i} y_i x_i && \text{Commutativity of scalar mult} \\
(\sum\limits_{i} x_i x_i') \beta &= \sum\limits_{i} y_i x_i \\
\beta &= (\sum\limits_{i} x_i x_i')^{-1} (\sum\limits_{i} y_i x_i) \\
      &= \hat{\beta}_{OLS}
\end{align*} %]]></script>

<p>The cleverness here is in observing that we can shuffle the order of the
multiplication within the summation since <script type="math/tex">x_i' \beta</script> is a scalar and scalar
multiplication is commutative, which then allows us to factor out the
multiplication by <script type="math/tex">\beta</script> from the summation.</p>

<h4 id="consistency-with-1-dimensional-results">Consistency with 1-dimensional results</h4>

<p>Does this formula agree with the 1-dimensional case above? Note that we wanted
to obtain <script type="math/tex">a, b</script>, where <script type="math/tex">\beta = (a \; b)'</script> and <script type="math/tex">x_i = (x_i \; 1)'</script>.</p>

<script type="math/tex; mode=display">\hat{\beta}_{OLS} = (\sum x_i x_i')^{-1} (\sum y_i x_i)</script>

<p>Observe</p>

<script type="math/tex; mode=display">% <![CDATA[
x_i x_i' = \begin{bmatrix} x_i \\ 1 \end{bmatrix} \begin{bmatrix} x_i && 1 \end{bmatrix} = \begin{bmatrix} x_i^2 && x_i \\ x_i && 1 \end{bmatrix} %]]></script>

<p>and</p>

<script type="math/tex; mode=display">y_i x_i = \begin{bmatrix} y_i x_i \\ y_i \end{bmatrix}</script>

<p>so</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{\beta}_{OLS} = \begin{bmatrix} \sum x_i^2 && \sum x_i \\ \sum x_i && n \end{bmatrix}^{-1} \begin{bmatrix} \sum y_i x_i \\ \sum y_i \end{bmatrix}. %]]></script>

<p>Recall the formula for the inverse of a 2x2 matrix:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{bmatrix} a && b \\ c && d \end{bmatrix}^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d && -b  \\ -c && a \end{bmatrix}. %]]></script>

<p>So we obtain</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\hat{\beta}_{OLS} &=  \frac{1}{n \sum x_i^2 - (\sum x_i)(\sum x_i)} \begin{bmatrix} n && - \sum x_i \\ - \sum x_i && \sum x_i^2 \end{bmatrix} \begin{bmatrix} \sum y_i x_i \\ \sum y_i \end{bmatrix} \\
    &= \frac{1}{n \sum x_i^2 - (\sum x_i)(\sum x_i)} \begin{bmatrix} n \sum y_i x_i - (\sum x_i) (\sum y_i) \\ - (\sum x_i) (\sum y_i x_i) + (\sum x_i^2)(\sum y_i) \end{bmatrix}.
\end{align*} %]]></script>

<p>and our estimator for <script type="math/tex">a</script> is</p>

<script type="math/tex; mode=display">\hat{a} = \frac{n \sum y_i x_i - (\sum x_i) (\sum y_i)}{n \sum x_i^2 - (\sum x_i)(\sum x_i)}</script>

<p>which matches our earlier results.</p>

<p>The moral here is that matrix manipulation is easier when we generalize to more
dimensions.</p>

<h1 id="linear-regression-matrix-representation">Linear regression (matrix representation)</h1>

<p>Let us finally move on to the matrix representation of the linear regression
problem. Define the least squares objective function:</p>

<script type="math/tex; mode=display">\sum\limits_{i} (y_i - x_i' \beta)^2</script>

<p>Consider the matrix and vector</p>

<script type="math/tex; mode=display">X = \begin{pmatrix} x_1' \\ x_2' \\ \vdots \\ x_n' \end{pmatrix} \quad \text{and} \quad y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}.</script>

<p>First, we wish to show that the least squares objective above can be written
<script type="math/tex">\norm{ y - X \beta}_{2}^{2}</script>. Recall that for any <script type="math/tex">v \in \R^n,
\norm{v}_{2}^{2} = \sum\limits_{i} v_i^2</script>.</p>

<p>Consider the <script type="math/tex">i</script>th entry of <script type="math/tex">y - X \beta</script>. Clearly, the <script type="math/tex">i</script>th entry of
<script type="math/tex">y</script> is <script type="math/tex">y_i</script>, and the <script type="math/tex">i</script>th entry of <script type="math/tex">X \beta</script> is <script type="math/tex">x_i' \beta</script> since</p>

<script type="math/tex; mode=display">X \beta = \begin{pmatrix} x_1' \\ \vdots \\ x_n' \end{pmatrix} \beta = \begin{pmatrix} x_1' \beta \\ \vdots \\ x_n' \beta \end{pmatrix}.</script>

<p>So the <script type="math/tex">i</script>th entry of <script type="math/tex">y - X \beta</script> is <script type="math/tex">y_i - x_i' \beta</script>, and by
definition of 2-norm we have shown equivalence to our least squares objective
function.</p>

<script type="math/tex; mode=display">\norm{y - X \beta}_{2}^{2} = \sum\limits_{i} (y_i - x_i' \beta)^2</script>

<p>Now, we want to minimize this formulation</p>

<script type="math/tex; mode=display">\min\limits_{\beta} \norm{y - X \beta}_{2}^{2}.</script>

<p>Let us take the gradient of our objective function <script type="math/tex">f(\beta) = \norm{y - X
\beta}_{2}^{2}</script> and set it equal to 0.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\beta} f &= \nabla_{\beta} \norm{y - X\beta}_{2}^{2} \\
                 &= \nabla_{\beta} [ (y-X\beta)'(y - X\beta) ] && \norm{v}_{2}^{2} = v'v \\
                 &= \nabla_{\beta} [y'y - (X\beta)'y - y' X \beta + (X \beta)' (X \beta) ] && \text{FOIL} \\
                 &= \nabla_{\beta} [y'y - 2 y' X \beta + \beta' X' X \beta] && v'w = w'v \\
                 &= 0 - \nabla_{\beta} 2 y'X \beta + \nabla_{\beta} \beta' X' X \beta && \nabla_{\beta} c = 0 \\
                 &= - 2 X'y + \nabla_{\beta} \beta' X' X \beta && \nabla_{\beta} v'\beta = v \\
\end{align*} %]]></script>

<p>Now, we claim that for any symmetric matrix <script type="math/tex">A</script>, <script type="math/tex">\nabla_{\beta} (\beta ' A
\beta) = 2A \beta</script>. (We will prove this claim in the next lecture.)</p>

<p>Using this claim, we can solve for <script type="math/tex">\beta</script> as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\beta} f &= 0 \\
-2 X'y + 2 X'X \beta &= 0 \\
X'X \beta &= X'y \\
\beta &= (X'X)^{-1} X'y
\end{align*} %]]></script>

<p>which may be familiar if you have a statistics background.</p>


  </div><a class="u-url" href="/notes/ece532/lecture-01" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Matthew Stone</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Matthew Stone</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/msto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">msto</span></a></li><li><a href="https://www.twitter.com/m_sto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">m_sto</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Under construction
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
