<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Algebraic, geometric, and probabilistic views of OLS | Matthew Stone</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Algebraic, geometric, and probabilistic views of OLS" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Administrative notes" />
<meta property="og:description" content="Administrative notes" />
<link rel="canonical" href="http://localhost:4002/notes/ece532/lecture-02" />
<meta property="og:url" content="http://localhost:4002/notes/ece532/lecture-02" />
<meta property="og:site_name" content="Matthew Stone" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-24T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Administrative notes","@type":"BlogPosting","url":"http://localhost:4002/notes/ece532/lecture-02","headline":"Algebraic, geometric, and probabilistic views of OLS","dateModified":"2019-01-24T00:00:00-06:00","datePublished":"2019-01-24T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4002/notes/ece532/lecture-02"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4002/feed.xml" title="Matthew Stone" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Matthew Stone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/notes/">Lecture notes</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Algebraic, geometric, and probabilistic views of OLS</h1>
    <p class="post-meta">
    , Lecture 2 &mdash;
      <time class="dt-published" datetime="2019-01-24T00:00:00-06:00" itemprop="datePublished">Jan 24, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="administrative-notes">Administrative notes</h1>

<ul>
  <li>Homework 1 will be uploaded by EOD today</li>
  <li>Readings will be assigned as the term progresses; for now, review chapter 3
in any of the references</li>
</ul>

<h1 id="summary">Summary</h1>

<p>Today in class we discussed the three views of ordinary least squares (OLS)
regression:</p>
<ol>
  <li>Algebraic (using matrices and gradients)</li>
  <li>Geometric (considering our solution to be a projection of <script type="math/tex">y</script> onto the
column space of <script type="math/tex">X</script>)</li>
  <li>Probabilistic (a generative model)</li>
</ol>

<script type="math/tex; mode=display">%% Latex helpers
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\L}{\mathcal{L}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\bigdot}{\boldsymbol{\cdot}}</script>

<h1 id="recap-ols">Recap: OLS</h1>

<p>Recall that our goal is to take a set of observations <script type="math/tex">\{(X_i,
y_i)\}_{i=1}^{n}</script>, where <script type="math/tex">X_i \in \R^p</script> and <script type="math/tex">y_i \in \R</script>, and find the
best fit for <script type="math/tex">y_i \approx X_i' \beta</script> for some regression vector <script type="math/tex">\beta</script>.
Our motivating example considered the case where <script type="math/tex">p=3</script>, <script type="math/tex">X_i</script> contained an
individual’s height, weight, and an intercept term, and <script type="math/tex">y_i</script> represented an
individual’s body mass index (BMI).</p>

<p>In matrix form:</p>

<script type="math/tex; mode=display">\hat{\beta}_{OLS} = \argmin\limits_{\beta} \norm{y - X\beta}_{2}^{2}</script>

<p>From this equation, we differentiated directly with respect to <script type="math/tex">\beta</script> and solved:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\beta} f &= \nabla_{\beta} \norm{y - X\beta}_{2}^{2} \\
                 &= \nabla_{\beta} [ (y-X\beta)'(y - X\beta) ] && \norm{v}_{2}^{2} = v'v \\
                 &= \nabla_{\beta} [y'y - (X\beta)'y - y' X \beta + (X \beta)' (X \beta) ] && \text{FOIL} \\
                 &= \nabla_{\beta} [y'y - 2 y' X \beta + \beta' X' X \beta] && v'w = w'v \\
                 &= 0 - \nabla_{\beta} 2 y'X \beta + \nabla_{\beta} \beta' X' X \beta && \nabla_{\beta} c = 0 \\
                 &= - 2 X'y + \nabla_{\beta} \beta' X' X \beta && \nabla_{\beta} v'\beta = v \\
\end{align*} %]]></script>

<p>Last class, we continued our solution by relying on the following claim:</p>

<script type="math/tex; mode=display">\nabla_{\beta} (\beta' A \beta) = 2 A \beta</script>

<p>if <script type="math/tex">A</script> is symmetric. To prove this claim, we want to think about both sides
of this equation, which are <script type="math/tex">p</script>-dimensional vectors, and show that the
<script type="math/tex">j</script>th components of each are equal.</p>

<p>On the left hand side (LHS), the <script type="math/tex">j</script>th component is the partial derivative
with respect to <script type="math/tex">\beta_j</script>:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial \beta_j} (\beta' A \beta) = \frac{\partial}{\partial \beta_j} \sum\limits_{k, l} \beta_k a_{kl} \beta_l</script>

<p>Here, we are letting <script type="math/tex">A = \begin{pmatrix} a_{kl} \end{pmatrix}</script>. Let
<script type="math/tex">a_{k \bigdot}</script> denote the <script type="math/tex">k</script>th row of <script type="math/tex">A</script>. To obtain the above
equivalence, recall that the <script type="math/tex">i</script>th entry of the matrix-vector product <script type="math/tex">Ax</script> is the dot product of the <script type="math/tex">i</script>th row of <script type="math/tex">A</script> with <script type="math/tex">x</script>, that is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
A \beta &= \begin{pmatrix} {a_{1\bigdot}}'\beta \\ \vdots \\ {a_{p \bigdot}}' \beta  \end{pmatrix} \\
        &= \begin{pmatrix} \sum\limits_{l} a_{1l} \beta_l \\ \vdots \\ \sum\limits_{l} a_{pl} \beta_l \end{pmatrix}
\end{align*} %]]></script>

<p>so we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\beta' A \beta &= \begin{pmatrix} \beta_1 && \cdots && \beta_p \end{pmatrix} \begin{pmatrix} \sum\limits_{l} a_{1l} \beta_l \\ \vdots \\ \sum\limits_{l} a_{pl} \beta_l \end{pmatrix} \\
              &= \beta_1 \sum\limits_l a_{1l} \beta_l + \cdots + \beta_p \sum\limits_{l} a_{pl} \beta_l \\
              &= \sum\limits_{k} \beta_k \sum\limits_l a_{kl} \beta_l \\
              &= \sum\limits_{k, l} \beta_k a_{kl} \beta_l
\end{align*} %]]></script>

<p>as above. Now let us continue with our proof:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\frac{\partial}{\partial \beta_j} (\beta' A \beta) &= \frac{\partial}{\partial \beta_j} \sum\limits_{k, l} \beta_k a_{kl} \beta_l \\
        &= \frac{\partial}{\partial \beta_j} \left( \sum\limits_{k \ne j} a_{kj} \beta_k \beta_j + a_{jj} \beta_{j}^2 + \sum\limits_{k \ne j} a_{jk} \beta_j \beta_k \right) \\
        &= \frac{\partial}{\partial \beta_j} \left( 2 \sum\limits_{k \ne j} a_{kj} \beta_k \beta_j + a_{jj} \beta_j^2 \right) && A \text{ is symmetric} \\
        &= 2 \sum\limits_{k \ne j} a_{kj} \beta_k + 2 a_{jj} \beta_j \\
        &= 2 \sum\limits_{k} a_{kj} \beta_k
\end{align*} %]]></script>

<p>Is this equal to the <script type="math/tex">j</script>th component of the right hand side (RHS) of our
equation, <script type="math/tex">2A\beta</script>? We showed above that</p>

<script type="math/tex; mode=display">A \beta = \begin{pmatrix} \sum\limits_{l} a_{1l} \beta_l \\ \vdots \\ \sum\limits_{l} a_{pl} \beta_l \end{pmatrix}</script>

<p>whose <script type="math/tex">j</script>th component is <script type="math/tex">\sum\limits_{l} a_{jl} \beta_l</script>. Twice this
quantity is indeed equal to the <script type="math/tex">j</script>th component of the LHS we derived
earlier, <script type="math/tex">2 \sum\limits_k a_{kj} \beta_k</script>.</p>

<p>Having shown that <script type="math/tex">\nabla_{\beta} (\beta' A \beta) = 2 A \beta</script>, we can
complete our derivation of the ordinary least squares estimate for <script type="math/tex">\beta</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nabla_{\beta} f &= - 2 X'y + \nabla_{\beta} \beta' X' X \beta \\
                 &= - 2 X'y + 2 X' X \beta \\
\end{align*} %]]></script>

<p>Setting this equal to 0, we can solve:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
- 2 X'y + 2 X' X \beta &= 0 \\
  X'X \beta &= X'y \\
  \beta &= (X'X)^{-1} X'y
\end{align*} %]]></script>

<p>which is our estimate <script type="math/tex">\hat{\beta}_{OLS}</script>.</p>

<h1 id="geometric-interpretation-of-ols">Geometric interpretation of OLS</h1>

<p>The OLS estimate finds a projection of <script type="math/tex">y \in \R^n</script> onto the column space of
<script type="math/tex">X</script>.</p>

<p>Observe that the columns of <script type="math/tex">X \in \R^{n \times p}</script> are <script type="math/tex">\{ X^1, \cdots,
X^p \}</script> and all <script type="math/tex">X^j \in \R^n</script>.</p>

<p>Recall that the column space of a matrix <script type="math/tex">X</script> is</p>

<script type="math/tex; mode=display">\text{Col} X = \text{span} \{X^1, \cdots, X^p\} = \{ \sum\limits_{j} \alpha_j X^j \mid \alpha_j \in \R \}</script>

<p>If we consider <script type="math/tex">X \beta</script>, for different choices of <script type="math/tex">\beta</script> we get different
linear combinations of the columns <script type="math/tex">\{ X^1, \cdots, X^p \}</script>. (Recall that the
matrix-vector product <script type="math/tex">Ax</script> can be computed as a linear combination of the
columns of <script type="math/tex">A</script> using the entries in <script type="math/tex">x</script> as weights, <script type="math/tex">Ax = \sum x_i A^i</script>.)</p>

<script type="math/tex; mode=display">% <![CDATA[
X \beta = \begin{pmatrix} X^1 && \cdots && X^p \end{pmatrix} \begin{pmatrix} \beta_1 \\ \vdots \\ \beta_p \end{pmatrix} = \sum\limits_{j} \beta_j X^j = \text{Col} X %]]></script>

<p>Therefore the set <script type="math/tex">\{ X \beta \mid  \beta \in \R^p \}</script> is exactly the set of
vectors in <script type="math/tex">\text{Col} X</script>.</p>

<p>Since the 2-norm is Euclidean distance, the <script type="math/tex">\hat{\beta} \in \R^p</script> that
minimizes <script type="math/tex">\norm{y - X\beta}_{2}^2</script> is finding the closest point <script type="math/tex">X
\hat{\beta} \in \text{Col} X</script>.</p>

<h1 id="probabilistic-interpretation-of-ols">Probabilistic interpretation of OLS</h1>

<p>Let us visit an idea that will recur throughout this course. Instead of
thinking in terms of data points <script type="math/tex">(X_i, y_i)</script> or matrices <script type="math/tex">(X, y)</script>,
consider a generative model. Consider the setting where the <script type="math/tex">(X_i, y_i)</script> are
generated in a random way:</p>

<script type="math/tex; mode=display">y_i = X_{i}' \beta + \varepsilon_i</script>

<p>where <script type="math/tex">X_i</script> is a fixed vector in <script type="math/tex">\R^p</script> and <script type="math/tex">\varepsilon_i \sim N(0,
\sigma^2)</script>. That is, begin with fixed data points <script type="math/tex">X_i</script>, generate random
error terms <script type="math/tex">\varepsilon_i</script>, and construct our <script type="math/tex">y_i</script>, then estimate
<script type="math/tex">\beta</script> using the observed and constructed <script type="math/tex">(X_i, y_i)</script>.</p>

<p>We can use maximum likelihood estimation (MLE) to find an estimate for
<script type="math/tex">\beta</script>. To recap MLE, we write the probability of observing <script type="math/tex">(X_i, y_i)</script>
assuming <script type="math/tex">\beta</script> was our true regression vector, and then maximize over
<script type="math/tex">\beta</script>.</p>

<p>Let us define a likelihood function (the probability of observing our data):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\L_{\beta}(X,y) &= \prod\limits_{i} P_{\beta} (X_i, y_i) && \text{Assuming independent observations} \\
                &= \prod\limits_{i} \frac{1}{\sqrt{2\pi} \sigma} \exp\left({\frac{- (y_i - x_i' \beta)^2}{2 \sigma^2}}\right) && \text{Using normal PDF}
\end{align*} %]]></script>

<p>To view how we obtained this, recall that the probability density function
(PDF) of a normally distributed random variable <script type="math/tex">X \sim N(\mu, \sigma^2)</script> is</p>

<script type="math/tex; mode=display">p(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left( { \frac{-(x - \mu)^2}{2\sigma^2} } \right)</script>

<p>and if <script type="math/tex">\mu = 0</script>, this becomes</p>

<script type="math/tex; mode=display">p(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left( { \frac{-x^2}{2\sigma^2} } \right)</script>

<p>Above we stated that <script type="math/tex">y_i = X_i' \beta + \varepsilon_i</script>. Rearranging, we have
<script type="math/tex">\varepsilon_i = y_i - X_i' \beta</script>, and we know from the definition of our
model that <script type="math/tex">\varepsilon_i \sim N(0, \sigma^2)</script>. We can therefore simply
substitute <script type="math/tex">y_i - X_i' \beta</script> for <script type="math/tex">x</script> in the normal PDF, and obtain the
above formulation</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\L_{\beta}(X,y) &= \prod\limits_{i} \frac{1}{\sqrt{2\pi} \sigma} \exp\left({\frac{- (y_i - x_i' \beta)^2}{2 \sigma^2}}\right) 
\end{align*} %]]></script>

<p>Now we want to maximize this likelihood function with respect to <script type="math/tex">\beta</script>.
First, let’s re-order our equation.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\L_{\beta}(X,y) &= \prod\limits_{i} \frac{1}{\sqrt{2\pi} \sigma} \exp\left({\frac{- (y_i - x_i' \beta)^2}{2 \sigma^2}}\right)  \\
                &= \left(\frac{1}{\sqrt{2\pi} \sigma} \right)^n \prod\limits_{i} \exp\left({\frac{- (y_i - x_i' \beta)^2}{2 \sigma^2}}\right) \\
                &= \left(\frac{1}{\sqrt{2\pi} \sigma} \right)^n \exp\left({\sum\limits_{i} \frac{- (y_i - x_i' \beta)^2}{2 \sigma^2}}\right) \\
                &= \left(\frac{1}{\sqrt{2\pi} \sigma} \right)^n \exp\left({ \frac{-1}{2\sigma^2} \sum\limits_{i} (y_i - x_i' \beta)^2}\right) \\
\end{align*} %]]></script>

<p>Note that the quantity <script type="math/tex">\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^n</script> is
fixed with respect to <script type="math/tex">\beta</script>. Additionally, the exponential function is
monotonically increasing, so to maximize the likelihood function we only need
to maximize the exponent. The quantity <script type="math/tex">\frac{-1}{2\sigma^2}</script> is also fixed
with respect to <script type="math/tex">\beta</script>, and its product with the always-positive sum of
squares will always be negative, so maximizing the exponent is equivalent to
minimizing the sum of squares. We have thus shown that maximizing the
likelihood function <script type="math/tex">\L_{\beta}</script> with respect to <script type="math/tex">\beta</script> is the same as
minimizing <script type="math/tex">\sum (y_i - X_i'\beta)^2</script>, which is our ordinary least squares
estimate.</p>

<p>Note that unlike our considerations of the algebraic and geometric forms of the
least squares problem, our probabilistic from required to make certain
assumptions about our data - namely that the data points were independent and
had Gaussian error terms. Despite this constraint, the generative model permits
us to use a whole family of estimators for different distributions, and permits
inference, such as constructing confidence intervals or performing hypothesis
tests.</p>

<h1 id="constructing-confidence-intervals">Constructing confidence intervals</h1>

<p>Suppose we ran OLS to obtain the estimate <script type="math/tex">\hat{\beta} = (X'X)^{-1}X'y</script>.</p>

<ol>
  <li>What is the best estimate for <script type="math/tex">\beta_j</script>? Obviously <script type="math/tex">\hat{\beta}_j</script>.</li>
  <li>But what if I want error bars for my estimate? Error bars can indicate
uncertainty or variability depending on the distribution of <script type="math/tex">\beta_j</script>.</li>
</ol>

<p>If we use the generative model <script type="math/tex">y = X \beta + \varepsilon</script>, then there is
some variability in <script type="math/tex">\hat{\beta}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\hat{\beta} &= (X'X)^{-1} X'y \\
            &= (X'X)^{-1} X' (X \beta + \varepsilon) \\
            &= (X'X)^{-1} X'X \beta + (X'X)^{-1} X' \varepsilon \\
            &= \beta + (X'X)^{-1} X' \varepsilon \\
\end{align*} %]]></script>

<p>Therefore <script type="math/tex">\hat{\beta}_j</script> is distributed as <script type="math/tex">\beta_j + [(X'X)^{-1}X' \varepsilon]_j</script>.
(Since <script type="math/tex">X</script> and <script type="math/tex">\beta</script> are fixed, note that the only randomness is coming
from our error terms <script type="math/tex">\varepsilon</script>.)</p>

<p>Intuitively, our confidence interval will then be <script type="math/tex">\beta_j \pm c \sqrt{v_j}</script>
for some constant <script type="math/tex">c</script>, where <script type="math/tex">v_j</script> is the <script type="math/tex">j</script>th component of the variance
of <script type="math/tex">(X'X)^{-1} X' \varepsilon</script>. We will explore this more in the next
lecture, but here’s a quick preview of the result:</p>

<script type="math/tex; mode=display">(X'X)^{-1}X' \varepsilon \sim N(0, \Sigma^2)</script>

<p>where <script type="math/tex">\Sigma^2 = \sigma^2 (X'X)^{-1}</script>. Therefore we should take <script type="math/tex">v_j =
\sigma^2 \cdot [(X'X)^{-1}]_j</script>.</p>

  </div><a class="u-url" href="/notes/ece532/lecture-02" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Matthew Stone</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Matthew Stone</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/msto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">msto</span></a></li><li><a href="https://www.twitter.com/m_sto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">m_sto</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Under construction
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
