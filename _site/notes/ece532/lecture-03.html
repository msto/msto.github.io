<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Confidence intervals | Matthew Stone</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Confidence intervals" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Administrative notes" />
<meta property="og:description" content="Administrative notes" />
<link rel="canonical" href="http://localhost:4002/notes/ece532/lecture-03" />
<meta property="og:url" content="http://localhost:4002/notes/ece532/lecture-03" />
<meta property="og:site_name" content="Matthew Stone" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-29T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Administrative notes","@type":"BlogPosting","url":"http://localhost:4002/notes/ece532/lecture-03","headline":"Confidence intervals","dateModified":"2019-01-29T00:00:00-06:00","datePublished":"2019-01-29T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4002/notes/ece532/lecture-03"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4002/feed.xml" title="Matthew Stone" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Matthew Stone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/notes/">Lecture notes</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Confidence intervals</h1>
    <p class="post-meta">
    , Lecture 3 &mdash;
      <time class="dt-published" datetime="2019-01-29T00:00:00-06:00" itemprop="datePublished">Jan 29, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="administrative-notes">Administrative notes</h1>

<ul>
  <li>Homework 1 is due next Tuesday (Feb 5) via Gradescope by 2:30pm</li>
  <li>No class next Tuesday (Feb 5)</li>
</ul>

<h1 id="summary">Summary</h1>

<p>Today in class we reviewed our discussion of ordinary least squares (OLS)
regression and discussed how to construct confidence intervals around the
entries of our estimated regression vector <script type="math/tex">\hat{\beta}_{OLS}</script>, primarily by
exploring the properties of the mean and covariance of a multivariate Gaussian
variable. We also began to introduce ridge regression.</p>

<script type="math/tex; mode=display">%% Latex helpers
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\L}{\mathcal{L}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Col}{\mathrm{Col}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\bigdot}{\boldsymbol{\cdot}}</script>

<h1 id="recap-ols">Recap: OLS</h1>

<p>We have shown how to solve the regression problem <script type="math/tex">\hat{\beta}_{OLS} = \argmin
\limits_{\beta} \norm{y - X\beta}_{2}^2</script> where our data is a matrix <script type="math/tex">X \in
\R^{n \times p}</script> and our observations are a vector <script type="math/tex">y \in \R^n</script>.</p>

<p>We have three notable takeaways from the past two lectures:</p>

<ol>
  <li>The formula for our estimator <script type="math/tex">\hat{\beta}_{OLS} = (X'X)^{-1}X'y</script></li>
  <li>The observation that <script type="math/tex">\{ X \beta : \beta \in \R^p \}</script> is the column space
of <script type="math/tex">X</script>, and therefore the estimate <script type="math/tex">\hat{\beta}_{OLS}</script>, which minimizes
the Euclidean distance from <script type="math/tex">y</script> to <script type="math/tex">X \beta</script>, corresponds to the
projection of <script type="math/tex">y</script> onto <script type="math/tex">\text{Col} X</script>.</li>
  <li>
    <p>When considering the probabilistic view (or generative model), the OLS
estimate is equivalent to the maximum likelihood estimate (MLE) if <script type="math/tex">y_i =
X_i' \beta + \varepsilon_i</script> where <script type="math/tex">\varepsilon_i
\stackrel{\text{i.i.d}}{\sim} N(0, \sigma^2)</script>. This is because we defined
the likelihood function</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 \L_{\beta}(X, y) &= P_{\beta}(X, y) \\
                  &= \prod\limits_{i} P_{\beta} (X_i, y_i) && \text{By our independence assumption} \\
                  &= \prod\limits_{i} \frac{1}{\sqrt{2\pi} \sigma} \exp \left( \frac{- (y_i - X_i'\beta)^2}{2\sigma^2} \right) && \text{By our Gaussian assumption}
 \end{align*} %]]></script>

    <p>so maximizing <script type="math/tex">\L</script> over <script type="math/tex">\beta</script> is equivalent to minimizing <script type="math/tex">\sum (y_i -
 X_i' \beta)^2</script> over <script type="math/tex">\beta</script>.</p>
  </li>
</ol>

<p>Why are we interested in the probabilistic model? For one, it permits us to
construct confidence intervals and perform hypothesis tests.</p>

<h1 id="confidence-intervals-for-beta_j">Confidence intervals for <script type="math/tex">\beta_j</script></h1>

<p>In the last lecture we introduced the intuition that our confidence interval
should be our estimate plus or minus some quantity times the standard deviation
of our estimate, or <script type="math/tex">\hat{\beta}_j \pm c \sqrt{v_j}</script> where <script type="math/tex">v_j =
\text{Var}(\hat{\beta}_j)</script> and <script type="math/tex">c</script> is some constant. But what are
<script type="math/tex">\text{Var}(\hat{\beta}_j)</script> and <script type="math/tex">c</script>? Well, it depends on the distribution
of <script type="math/tex">\hat{\beta}_j</script>.</p>

<p>From our formula, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\hat{\beta} &= (X'X)^{-1} X' y \\
            &= (X'X)^{-1} X' (X \beta + \varepsilon) \\
            &= \beta + (X'X)^{-1} X' \varepsilon
\end{align*} %]]></script>

<p>Note that the only source of randomness in our estimator is <script type="math/tex">\varepsilon</script>, as
<script type="math/tex">\beta</script> and <script type="math/tex">X</script> are both fixed. If <script type="math/tex">\varepsilon</script> is a Gaussian vector,
then <script type="math/tex">(X'X)^{-1} X' \varepsilon</script> is also Gaussian, since <script type="math/tex">A\varepsilon</script> is
Gaussian if <script type="math/tex">A</script> is a fixed matrix. Each <script type="math/tex">\varepsilon_i \sim N(0,
\sigma^2)</script>, so <script type="math/tex">\varepsilon</script> is a multivariate Gaussian.</p>

<p>As a reminder, we say <script type="math/tex">x \sim N(\mu, \Sigma)</script> is a multivariate Gaussian
vector in <script type="math/tex">\R^n</script> with mean <script type="math/tex">\mu \in \R^n</script> and covariance <script type="math/tex">\Sigma \in \R^{n
\times n}</script> if it has the probability density function</p>

<script type="math/tex; mode=display">p(x) = \frac{1}{\sqrt{\det(2 \pi \Sigma)}} \exp \left( \frac{-1}{2} (x - \mu)'  \Sigma^{-1} (x - \mu) \right)</script>

<p>So we can say that <script type="math/tex">\varepsilon \sim N(0, \sigma^2 I_n)</script>. It now remains to
compute the mean <script type="math/tex">\mu</script> and covariance <script type="math/tex">\Sigma</script> of <script type="math/tex">(X'X)^{-1}X'
\varepsilon</script>.</p>

<p>Let’s begin by simplifying the situation and calculating <script type="math/tex">\mu</script> and <script type="math/tex">\Sigma</script>
for <script type="math/tex">A\varepsilon</script>, where <script type="math/tex">A \in \R^{p \times n}</script> is some fixed matrix. We
will make two claims:</p>

<ol>
  <li>The mean is <script type="math/tex">\E[A\varepsilon] = A \E[\varepsilon]</script></li>
  <li>The covariance is <script type="math/tex">\Cov[A \varepsilon] = A' \Cov[\varepsilon] A</script></li>
</ol>

<p>This matches what we would expect intuitively from the 1-dimensional
equivalents:</p>

<ol>
  <li>The mean <script type="math/tex">\E[ax] = a \E[x]</script></li>
  <li>The variance <script type="math/tex">\Var[ax] = a^2 \Var[x]</script></li>
</ol>

<h2 id="claim-1">Claim 1</h2>

<p>Let’s first verify claim 1, <script type="math/tex">\E[A\varepsilon] = A \E[\varepsilon]</script>. We will
treat <script type="math/tex">\E</script> as an entrywise function, i.e., <script type="math/tex">% <![CDATA[
\E[x] = \begin{pmatrix} E[x_1]
&& \cdots && E[x_n] \end{pmatrix}' %]]></script>.</p>

<p>As with our earlier proofs of equality, we want to show that these vectors are
entry-wise equal, so we will consider the <script type="math/tex">j</script>th entry of both sides of the
equation.</p>

<p>On the LHS, we have the expected value of the <script type="math/tex">j</script>th entry of <script type="math/tex">A\varepsilon</script>, <script type="math/tex">a_j' \varepsilon</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\E[(A\varepsilon)_j] &= \E [a_j' \varepsilon] && \text{where $a_j'$ is $j$th row of $A$} \\
                     &= \E [ \sum \limits_i a_{ji} \varepsilon_i ] \\
                     &= \sum\limits_i \E[a_{ji} \varepsilon_i] && \text{Linearity of expected value} \\
                     &= \sum\limits_i a_{ji} \E[\varepsilon_i] && \text{Shown above}
\end{align*} %]]></script>

<p>On the RHS, we have the <script type="math/tex">j</script>th entry of <script type="math/tex">A \begin{pmatrix} \E[\varepsilon_1]
\\ \vdots \\ \E[\varepsilon_n] \end{pmatrix}</script>, which is</p>

<script type="math/tex; mode=display">a_j' \begin{pmatrix} \E[\varepsilon_1] \\ \vdots \\ \E[\varepsilon_n] \end{pmatrix} = \sum \limits_i a_{ji} \E[\varepsilon_i]</script>

<p>So we have shown <script type="math/tex">\E[A\varepsilon] = A \E[\varepsilon]</script>. Importantly in our
case, since <script type="math/tex">\E[\varepsilon]= 0</script>, we have <script type="math/tex">E[A\varepsilon] = A
\E[\varepsilon] = 0</script>.</p>

<h2 id="claim-2">Claim 2</h2>

<p>Now let us verify claim 2, <script type="math/tex">\Cov[A \varepsilon] = A' \Cov[\varepsilon] A</script>.</p>

<p>A quick review of covariance - for any <script type="math/tex">z \in \R^n</script>, the covariance matrix is
a matrix <script type="math/tex">\Sigma \in \R^{n \times n}</script>, where <script type="math/tex">\Sigma_{ij} = \Cov[z_i, z_j] =
\E[z_i z_j] - \E[z_i]\E[z_j]</script>. If <script type="math/tex">i=j</script>, then <script type="math/tex">\Cov[z_i, z_j] = \Var(z_i)</script>.</p>

<p>We can make a useful observation: <script type="math/tex">\Cov[z] = \E[z z'] - \E[z]\E[z]'</script>. Why?
Look at the RHS. The <script type="math/tex">ij</script>th entry of <script type="math/tex">(zz')</script> is <script type="math/tex">z_i z_j</script>. Since the
expected value is an element-wise function, the <script type="math/tex">ij</script>th entry of <script type="math/tex">\E[zz']</script>
is therefore <script type="math/tex">E[z_i z_j]</script>.</p>

<p>Also, the <script type="math/tex">ij</script>th entry of <script type="math/tex">\E[z]\E[z]'</script> is the <script type="math/tex">ij</script>th entry of
<script type="math/tex">% <![CDATA[
\begin{pmatrix} \E[z_1] \\ \vdots \\ \E[z_n] \end{pmatrix} \begin{pmatrix}
\E[z_1] && \cdots && \E[z_n]\end{pmatrix} %]]></script>. By the same logic, the <script type="math/tex">ij</script>th
entry of this matrix is <script type="math/tex">\E[z_i] \E[z_j]</script>.</p>

<p>Therefore the <script type="math/tex">ij</script>th entry of <script type="math/tex">\E[zz'] - \E[z]E[z]'</script> is <script type="math/tex">\E[z_i z_j] -
\E[z_i] \E[z_j]</script>, which is exactly <script type="math/tex">\Cov[z_i, z_j]</script> and the <script type="math/tex">ij</script>th entry
of <script type="math/tex">\Cov[z]</script>.</p>

<p>Using this observation, we can prove claim 2:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\Cov[A \varepsilon] &= \E [ (A \varepsilon) (A \varepsilon)'] - \E [A \varepsilon] (\E [A \varepsilon])' \\
                    &= \E [ A \varepsilon \varepsilon' A'] && \E [A \varepsilon] = 0 \\
                    &= A \E [\varepsilon \varepsilon' A'] && \text{Claim 1} \\
                    &= A \E [\varepsilon \varepsilon'] A' && \text{Claim 1}
\end{align*} %]]></script>

<p>Note that <script type="math/tex">\E[\varepsilon \varepsilon']</script> is <script type="math/tex">\Cov [\varepsilon]</script>, since
<script type="math/tex">\Cov[\varepsilon] = \E [ \varepsilon \varepsilon'] -\E [\varepsilon]
\E[\varepsilon]'</script> and <script type="math/tex">\E [\varepsilon] = 0</script>. Substituting in, we then have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\Cov[A \varepsilon] &= A \Cov [\varepsilon] A' \\
                    &= A \sigma^2 I A' \\
                    &= \sigma^2 AA'
\end{align*} %]]></script>

<p>In general, we have <script type="math/tex">\Cov[Az] = A \E[zz'] A' - A \E[z]E[z'] A'</script> if <script type="math/tex">\E[z]
\ne 0</script>.</p>

<p>Returning to our formula <script type="math/tex">\hat{\beta} = \beta + (X'X)^{-1}X' \varepsilon</script>, we
have <script type="math/tex">\hat{\beta} \sim N(\beta, \sigma^2 AA')</script> where <script type="math/tex">A = (X'X)^{-1}X'</script>.
Note that <script type="math/tex">AA' = (X'X)^{-1}X'X (X'X)^{-T} = (X'X)^{-1}</script>, so we finally have</p>

<script type="math/tex; mode=display">\hat{\beta}_{OLS} \sim N(\beta, \sigma^2 (X'X)^{-1})</script>

<h2 id="constructing-the-ci">Constructing the CI</h2>

<p>Recall that the big picture goal is to create a confidence interval for <script type="math/tex">\hat{\beta}_j</script>.</p>

<p>So if we want to construct such a CI, the variance <script type="math/tex">\Var[\hat{\beta}_j]</script> is
just <script type="math/tex">\sigma^2 v_j</script>, where <script type="math/tex">v_j</script> is the <script type="math/tex">jj</script>th entry of <script type="math/tex">(X'X)^-1</script>, and
the CI is</p>

<script type="math/tex; mode=display">\hat{\beta}_j \pm z_{\alpha / 2} \sqrt{\sigma^2 v_j}</script>

<p>where <script type="math/tex">z_{\alpha / 2}</script> is the quantile of the standard normal distribution
corresponding to a given <script type="math/tex">\alpha</script>. For example, in a 95% CI, <script type="math/tex">\alpha = 1 -
0.95 = 0.05</script>.</p>

<p>Intuitively, we know that in the standard normal distribution, <script type="math/tex">z_{\alpha/2}</script>
standard deviations outside the mean contain <script type="math/tex">(1-\alpha)\%</script> of the data. So
to construct a confidence interval, we shift the center of the standard normal
by our estimate for the mean, <script type="math/tex">\hat{\beta}_j</script>, and scale the size of the
standard deviation by an estimate for the SD in our data, <script type="math/tex">\sqrt{\sigma^2
v_j}</script>.</p>

<p>What if we don’t know <script type="math/tex">\sigma</script> (which we don’t)? All the other parameters are
available, but we can’t know <script type="math/tex">\sigma</script> ahead of time. We can estimate it using
the sum of squared residuals:</p>

<script type="math/tex; mode=display">\hat{\sigma} = \sqrt{\frac{1}{n - p} \sum \limits_i (y_i - X_i'\hat{\beta})^2} = \sqrt{\frac{1}{n - p} \norm{y - X \beta}_{2}^2}</script>

<p>Note that for large <script type="math/tex">n</script>, <script type="math/tex">\frac{1}{n} \approx \frac{1}{n - p}</script>.</p>

<p>So our final formula for the confidence interval is</p>

<script type="math/tex; mode=display">\hat{\beta}_j \pm z_{\alpha / 2} \hat{\sigma} \sqrt{v_j}</script>

<p>The point here is that generative models let us quantify uncertainty.</p>

<h1 id="ridge-regression">Ridge regression</h1>

<p>Ridge regression is a variant of OLS. So far, we have assumed <script type="math/tex">X'X</script> is
invertible. What if it isn’t? Intuitively, in this case, multiple solutions
exist to the minimization problem <script type="math/tex">\min \limits_{\beta} \norm{y - X
\beta}_{2}^2</script>.</p>

<p>Is it reasonable to assume <script type="math/tex">X'X</script> is invertible? Sometimes, but observe that
if the columns of <script type="math/tex">X</script> are not linearly independent, then <script type="math/tex">X'X</script> is not
invertible. We can show this with some clever algebra.</p>

<p>If the columns of <script type="math/tex">X</script> are not linearly independent, then there exists a
linear combination of the columns <script type="math/tex">\sum v_i X_i = 0</script> with <script type="math/tex">v_i</script> not all
zero. This corresponds to a matrix vector product <script type="math/tex">Xv = 0</script>i where <script type="math/tex">v =
\begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \ne 0</script>.</p>

<p>Now suppose <script type="math/tex">(X'X)</script> is invertible. If we left-multiply <script type="math/tex">Xv</script> by <script type="math/tex">(X'X)^{-1}X'</script>, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
Xv &= 0 \\
(X'X)^{-1}X'Xv &= (X'X)^{-1}X' \cdot 0 \\
v &= 0
\end{align*} %]]></script>

<p>which is a contradiction.</p>

<p>From a different perspective, if the columns of <script type="math/tex">X</script> are not linearly
independent, the projection of <script type="math/tex">y</script> onto <script type="math/tex">\Col X</script> might not have a unique
representation. Therefore <script type="math/tex">\hat{\beta}_{OLS}</script> can’t be unique and the form
<script type="math/tex">\hat{\beta}_{OLS} = (X'X)^{-1}X' y</script> can’t be correct.</p>

<p>The conclusion is that if the columns are not linearly independent, we have a
problem.</p>

<p>Consider the high-dimensional setting where <script type="math/tex">p>n</script> and <script type="math/tex">X</script> is a “wide”
matrix. The columns of <script type="math/tex">X</script> cannot be linearly independent, because there are
<script type="math/tex">p</script> columns in <script type="math/tex">n</script>-dimensional space, where we can obtain a maximum of
<script type="math/tex">n</script> linearly independent vectors. So <script type="math/tex">(X'X)</script> will never be invertible in
this setting.</p>

<p>What do we do? Well, we have a “simple” “fix”, called ridge regression.</p>

<script type="math/tex; mode=display">\hat{\beta}_{\text{ridge}} = (X'X + \lambda I)^{-1} X'y</script>

<p>We add a multiple of the identity matrix to make the matrix invertible. We rely
on the fact that <script type="math/tex">X'X + \lambda I</script> is always invertible for any <script type="math/tex">\lambda >
0</script>, because <script type="math/tex">X'X + \lambda I</script> is a positive definite matrix. We will review
positive definite matrices in more detail later when we discuss the singular
value decomposition and dimensionality reduction, but for now, to verify
whether a matrix <script type="math/tex">A</script> is positive definite, check that <script type="math/tex">v' A V > 0 \forall v
\ne 0</script>.</p>

<p>In the next class we will explore ridge regression further.</p>

  </div><a class="u-url" href="/notes/ece532/lecture-03" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Matthew Stone</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Matthew Stone</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/msto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">msto</span></a></li><li><a href="https://www.twitter.com/m_sto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">m_sto</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Under construction
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
