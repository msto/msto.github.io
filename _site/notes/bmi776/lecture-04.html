<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Lecture 04 | Matthew Stone</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Lecture 04" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summary" />
<meta property="og:description" content="Summary" />
<link rel="canonical" href="http://localhost:4002/notes/bmi776/lecture-04" />
<meta property="og:url" content="http://localhost:4002/notes/bmi776/lecture-04" />
<meta property="og:site_name" content="Matthew Stone" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-31T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Summary","@type":"BlogPosting","url":"http://localhost:4002/notes/bmi776/lecture-04","headline":"Lecture 04","dateModified":"2019-01-31T00:00:00-06:00","datePublished":"2019-01-31T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4002/notes/bmi776/lecture-04"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4002/feed.xml" title="Matthew Stone" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Matthew Stone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/notes/">Lecture notes</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 04</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-01-31T00:00:00-06:00" itemprop="datePublished">Jan 31, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="summary">Summary</h2>

<p>Today in class we reviewed and finished our discussion of applying EM to the
motif finding problem, and introduced a second approach - Gibbs sampling.</p>

<script type="math/tex; mode=display">%% Latex helpers
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\card}[1]{\left\vert{#1}\right\vert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\L}{\mathcal{L}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bigdot}{\boldsymbol{\cdot}}</script>

<h2 id="announcements">Announcements</h2>

<ul>
  <li>Read the Gibbs paper for next class (Lawrence et al.)</li>
  <li>Explore the ipython notebook with examples of the gamma function and the
Dirichlet distribution</li>
</ul>

<h2 id="lecture-goals">Lecture goals</h2>

<p>We’ve discussed the OOPS (One Occurrence Per Sequence) and ZOOPS (Zero Or more
Occurrences Per Sequence) models for EM. Now, we want to extend to any number
of motifs and handle motifs of uncertain width.</p>

<p>Additionally, we want to choose our initial parameters intelligently. Can we
incorporate background information, so not all motif models are equally likely?</p>

<h2 id="meme">MEME</h2>
<h1 id="review-em-algorithm">Review EM algorithm</h1>

<p>Recall that in the EM algorithm, we begin our search from an initial estimate
of the parameters <script type="math/tex">p^{(0)}</script>. However, EM can only obtain a local maximum of
the likelihood function reachable from <script type="math/tex">p^{(0)}</script>. One approach to find the
global maximum could be to take multiple initializations of <script type="math/tex">p^{(0)}</script> and
choose the best final result. Unfortunately, in the high-dimensional setting, it is
computationally infeasible to exhaustively search this space.</p>

<p>However, there are some easy heuristics that can get us close to the global
maximum. We can observe that the true motif must be similar to some subsequence
in our set of sequences. Intuitively, the motif can’t be a sequence that isn’t
represented in our dataset. This insight suggests the following strategy:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For each distinct subsequence S with length W
    Derive p_(0) from S
    Run EM for 1 iteration
End
Choose the motif model that yielded the highest likelihood
Run EM from the corresponding p_(0) until convergence
</code></pre></div></div>

<p>This heuristic generally approximates the global maximum well.</p>

<h2 id="learning-sequence-motifs-with-gibbs-sampling">Learning Sequence Motifs with Gibbs Sampling</h2>

<h1 id="goals">Goals</h1>
<ul>
  <li>Discuss Gibbs sampling, a specific instance of Markov chain Monte Carlo</li>
  <li>Apply Gibbs sampling to the motif funding problem. This is a stochastic
method, in contrast to the deterministic MEME and its fuzzy heuristic.</li>
  <li>Parameter tying (for example, when assuming a palindromic motif)</li>
  <li>Priors on our model (incorporating background information, such as the
knowledge that some amino acids behave similarly)</li>
</ul>

<h1 id="motivation-for-gibbs-sampling">Motivation for Gibbs sampling</h1>
<p>Deterministic algorithms, like EM, can get stuck in local maxima. Gibbs
sampling searches with randomness to escape local maxima, and can be thought of
as a stochastic analog of EM.</p>

<h1 id="gibbs-sampling-approach">Gibbs sampling approach</h1>

<p>In EM, we maintained a distribution <script type="math/tex">Z_{i}^{(t)}</script>a over the possible motif
start positions. In Gibbs sampling, we will maintain a specific motif start
position <script type="math/tex">a_i</script> for each sequence, but will randomly resample <script type="math/tex">a_i</script> at each
iteration.</p>

<p>The algorithm is as follows:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: motif width W, set of sequences X
Initialize a random vector of motif start positions a
do
    Pick any sequence X_i
    Estimate p with the current motif locations a (holding out a_i)
    Compute the probability that the motif starts at each position in X_i
    Randomly sample a new motif start position a_i for X_i from this distribution
until convergence
</code></pre></div></div>

<h1 id="markov-chain-monte-carlo">Markov chain Monte Carlo</h1>
<p>Markov chains representing transition probabilities have a stationary
distribution if after sufficient iterations, <script type="math/tex">p^{(t)}(\text{state})</script>
converges to a stable distribution and approximates the actual probability of
being in that state. Mathematically,</p>

<script type="math/tex; mode=display">p^{(t)}(u) \approx p^{(t +1)}(u)</script>

<p>In matrix form, let <script type="math/tex">T</script> be the transition matrix such that <script type="math/tex">T_{uv} = \tau(u
\mid v)</script>, where <script type="math/tex">\tau(u \mid v)</script> is the transition probability of moving
from state <script type="math/tex">u</script> to state <script type="math/tex">v</script>. Let <script type="math/tex">p^{(t)}</script> denote the vector of
probabilities of being in a given state. Then the stationary distribution has
the form:</p>

<script type="math/tex; mode=display">p^{(t+1)} = T p^{(t)}</script>

<p>Note that this formulation is consistent with the intuition that <script type="math/tex">p^{(t+1)}(u) =
\sum\limits_{v} p^{(t)}(v) \tau(u \mid v)</script>.</p>

<p>So the states of our Markov chain correspond to some configuration of our
probabilistic model. In motif finding, this corresponds to some configuration
of our motif start positions, <script type="math/tex">a</script>. The transitions correspond to a change in
our start positions. For our purposes, a transition is permitted only if it
corresponds to changing the motif start position in a single sequence (i.e.,
updating multiple sequences in a single iteration is not permitted.)</p>

<h1 id="what-does-this-get-us">What does this get us?</h1>

<p>We’re trying to take some distribution <script type="math/tex">P(a)</script>, and we want to find the mode
of this distribution, which corresponds to the configuration of <script type="math/tex">a</script> that
maximizes <script type="math/tex">P(a)</script>. We’d also like to sample from this distribution.</p>

<p>In large models, it is difficult to do either of these directly. MCMC works
around this. The main idea is that we construct a Markov chain where the states
correspond to configurations of the motif positions <script type="math/tex">a</script>, and the stationary
distribution of this chain is the distribution <script type="math/tex">P(a)</script> of interest. So we need
to define a transition probability matrix <script type="math/tex">T</script> that will give us the desired
stationary distribution.</p>

<h1 id="building-a-markov-chain-with-the-desired-stationary-distribution">Building a Markov chain with the desired stationary distribution</h1>

<p>So how do we do this? We set our transition probabilities so that a property
called <em>detailed balance</em> holds. This guarantees a stationary distribution.</p>

<p>Transition probabilities satisfy detailed balance if the following is true:</p>

<script type="math/tex; mode=display">P(u) \tau (v \mid u) = P(v) \tau(u \mid v)</script>

<p>Note that if we assume detailed balance, we can show that our distribution is stationary:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\sum\limits_{v} P(u) \tau(v \mid u) &= \sum\limits_{v} P(v) \tau (u \mid v) \\
P(u) \sum\limits_{v} \tau(v \mid u) &= \sum\limits_{v} P(v) \tau (u \mid v) && \text{$P(u)$ constant for all $v$} \\
P(u) &= \sum\limits_{v} P(v) \tau (u \mid v) && \text{Sum of transition probs is 1} 
\end{align*} %]]></script>

<p>which matches our early definition of a stationary distribution, <script type="math/tex">P=TP</script>.</p>

<p>Suppose we draw <script type="math/tex">n</script> samples from such a Markov chain, and let
<script type="math/tex">\mathrm{count}(u)</script> denote the number of times the chain was in state <script type="math/tex">u</script>.
Then <script type="math/tex">\lim\limits_{n \to \infty} \frac{1}{n} \mathrm{count}(u) = P(u)</script>.</p>

<p>Gibbs sampling is a specific choice of our transition probabilities <script type="math/tex">T</script>. It
is a special case where we only permit transitions that change a single
variable, say <script type="math/tex">a_{\Delta}</script>, and we define a transition probability <script type="math/tex">\tau</script>
to be the conditional probability <script type="math/tex">P(a_{\Delta} \mid a_{i \ne \Delta})</script>.</p>

<p>Why does this choice of <script type="math/tex">T</script> give us detailed balance? Suppose we have two
states, or configurations of our motif start positions, that differ for only a
single sequence, <script type="math/tex">X_i</script>. Let’s call them <script type="math/tex">a</script> and <script type="math/tex">a'</script>, and let’s denote
the set of all possible sequence indices as <script type="math/tex">I</script>, so <script type="math/tex">a_{i} \ne a_{i}'</script>, but
<script type="math/tex">a_{I \setminus i} = a_{I \setminus i }'</script>. (Also let <script type="math/tex">A</script> denote the random variable
corresponding to our motif start positions.)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&= P(A = a) P(A_i = a_i' \mid A_{I \setminus i} = a_{I \setminus i}) \\
&= P(A_i = a_i \mid A_{I \setminus i} = a_{I \setminus i}) P(A_{I \setminus i} = a_{I \setminus i}) P(A_i = a_i' \mid A_{I \setminus i} = a_{I \setminus i}) \\
&= P(A_i = a_i \mid A_{I \setminus i} = a_{I \setminus i}) P(A_i = a_i', A_{I \setminus i} = a_{I \setminus i}) \\
&= P(A_i = a_i \mid A_{I \setminus i} = a_{I \setminus i}) P(A = a') \\
\end{align*} %]]></script>

<p>which is our definition of detailed balance.</p>

<h1 id="probability-of-being-in-a-state">Probability of being in a state</h1>

<p>We can define the probability of being in a state <script type="math/tex">u</script> as follows:</p>

<script type="math/tex; mode=display">P(u) \propto \prod\limits_{c} \prod\limits_{j=1}^{W} \left ( \frac{p_{c,j}}{p_{c,0}} \right ) ^{n_{c,j}(u)}</script>

<p>where <script type="math/tex">n_{c, j}(u)</script> is the number of times character <script type="math/tex">c</script> was observed at
position <script type="math/tex">j</script> across all motifs, given the motif start positions in state
<script type="math/tex">u</script>. So we’re taking the product of the probabilities of observing character
<script type="math/tex">c</script> at position <script type="math/tex">j</script> in our motif model over the probabilities from our
background model.</p>

<p>The full derivation can be found in Liu et al. (JASA 1995) but we can show that
this is proportional to the likelihood of our data to obtain some intuition for
this formula.</p>

<p>Remember that the state <script type="math/tex">u</script> is some configuration of our hidden variables,
the motif start positions in <script type="math/tex">A</script>, and we’re interested in the probability of
these start positions given our observed data, <script type="math/tex">P(A \mid X)</script>. (Note that in
the lecture notes for MEME, we used <script type="math/tex">Z</script> instead of <script type="math/tex">X</script> to refer to our
data.) By Bayes’ Theorem, we have</p>

<script type="math/tex; mode=display">P(A \mid X) = \frac{P(X \mid A) P(A)}{P(X)}</script>

<p>Since <script type="math/tex">X</script> is fixed, that means our probability is proportional to the numerator</p>

<script type="math/tex; mode=display">P(A \mid X) \propto P(X \mid A) P(A)</script>

<p>And if we assume a uniform prior over the possible motif start positions, that
means <script type="math/tex">P(A)</script> is constant and this is simply proportional to the probability
of the data given the motif start positions:</p>

<script type="math/tex; mode=display">P(A \mid X) \propto P(X \mid A)</script>

<p>Now, without being too rigorous, since we’re just trying to provide intuition
for the formula above, <script type="math/tex">P(X \mid A)</script> is some product of character
probabilities:</p>

<script type="math/tex; mode=display">P(X \mid A) = P(\text{all motif positions according to our motif model}) P(\text{all non-motif positions according to our background model})</script>

<p>We can multiply this by the fraction <script type="math/tex">\frac{P(\text{all motif positions according to our background model})}{P(\text{all motif positions according to our background model})}</script>, which is 1, and obtain</p>

<script type="math/tex; mode=display">P(X \mid A) = \frac{P(\text{all motif positions according to our motif model})}{P(\text{all motif positions according to our background model})} P(\text{all positions according to our background model})</script>

<p>Note that the second quantity is our null model and is independent of our motif
model, so it is constant, and <script type="math/tex">P(X\mid A)</script> is then proportional to the first
quantity:</p>

<script type="math/tex; mode=display">P(X \mid A) \propto \frac{P(\text{all motif positions according to our motif model})}{P(\text{all motif positions according to our background model})}</script>

<p>which is consistent with the formula above for the probability of a state in
the stationary distribution of our Markov Chain.</p>

<p>Next lecture we will discuss transition probabilities.</p>

  </div><a class="u-url" href="/notes/bmi776/lecture-04" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Matthew Stone</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Matthew Stone</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/msto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">msto</span></a></li><li><a href="https://www.twitter.com/m_sto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">m_sto</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Under construction
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
