<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Lecture 02 | Matthew Stone</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Lecture 02" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summary" />
<meta property="og:description" content="Summary" />
<link rel="canonical" href="http://localhost:4002/notes/bmi776/lecture-02" />
<meta property="og:url" content="http://localhost:4002/notes/bmi776/lecture-02" />
<meta property="og:site_name" content="Matthew Stone" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-24T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Summary","@type":"BlogPosting","url":"http://localhost:4002/notes/bmi776/lecture-02","headline":"Lecture 02","dateModified":"2019-01-24T00:00:00-06:00","datePublished":"2019-01-24T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4002/notes/bmi776/lecture-02"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4002/feed.xml" title="Matthew Stone" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Matthew Stone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/notes/">Lecture notes</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 02</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-01-24T00:00:00-06:00" itemprop="datePublished">Jan 24, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="summary">Summary</h1>

<p>Today in class we discussed the motif finding problem and began exploring the
application of expectation-maximization to it.</p>

<script type="math/tex; mode=display">%% Latex helpers
\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\card}[1]{\left\vert{#1}\right\vert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\L}{\mathcal{L}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bigdot}{\boldsymbol{\cdot}}</script>

<h1 id="the-motif-finding-problem">The motif finding problem</h1>

<p>A <em>motif</em> is some pattern occurring across a set of sequences that contains
some biological relevance. For example, motifs commonly occur in DNA at protein
binding sites.</p>

<p>The specific model learning task for the motif finding problem takes as input a
set of sequences of interest which putatively contain a motif. The goal is to
1) infer the motif model, i.e., the pattern with biological relevance, and 2)
identify the location of the motif within each input sequence.</p>

<p>We care about this problem because the genomic space is vast and we are
interested in identifying functional regions.</p>

<h1 id="motif-representation">Motif representation</h1>

<p>One approach to representing motifs is to use a <em>position weight matrix</em>. Such
a matrix has one row for every possible character in the biological alphabet
and one column for each entry in the sequence. The <script type="math/tex">ij</script>th entry of this
matrix represents the probability of observing base <script type="math/tex">i</script> at position <script type="math/tex">j</script>.</p>

<p>How can we construct a motif if our sequences aren’t aligned, and if we don’t
even know what the motif looks like?</p>

<h1 id="expectation-maximization">Expectation-maximization</h1>

<p>Expectation-maximization (EM) can be applied to solve probabilistic models that
include some hidden state. In our setting, the hidden state is the location of
the motif start site within each sequence. Other applications that we have seen
in the past include the Baum-Welch algorithm for solving hidden Markov models,
and clustering with Gaussian mixture models.</p>

<p>EM is an optimization algorithm to find the maximum likelihood solution for a
given probabilistic model. Formally, we wish to find the parameters <script type="math/tex">\theta</script>
with highest likelihood given our observed data:</p>

<script type="math/tex; mode=display">\theta_{ML} = \argmax\limits_{\theta} P(D \mid \theta, M)</script>

<p>where <script type="math/tex">M</script> is our model, the previously defined position weight matrix.</p>

<p>EM is useful because it is difficult to directly optimize the quantity <script type="math/tex">P(D
\mid \theta)</script>. We can decompose the likelihood by introducing hidden
information <script type="math/tex">Z</script> and creating a quantity that is easier to optimize:</p>

<script type="math/tex; mode=display">Q (\theta \mid \theta^+) = \sum \limits_{Z} P(Z \mid D, \theta^+) \log P(D, Z \mid \theta)</script>

<p>where <script type="math/tex">\theta^+</script> is some fixed setting of <script type="math/tex">\theta</script>. (TODO: review Durbin
11.6 for the derivation of this)</p>

<p>So we can define four steps to apply EM:</p>

<ol>
  <li>Define a model and a likelihood function</li>
  <li>Identify the hidden variables <script type="math/tex">Z</script></li>
  <li>Write the expectation step (E-step; compute the expectation <script type="math/tex">E[Z]</script> given current parameters)</li>
  <li>Write the maximization step (M-step; compute the parameters which maximize <script type="math/tex">Q</script> given the current state of our hidden <script type="math/tex">Z</script>)</li>
</ol>

<h1 id="meme---multiple-em-for-motif-elicitation">MEME - Multiple EM for Motif Elicitation</h1>

<h4 id="defining-our-model">Defining our model</h4>

<p>We define a motif to have a fixed width <script type="math/tex">w</script> and represent it with a
probability matrix <script type="math/tex">M \in \R^{\card{A} \times w + 1}</script>, where <script type="math/tex">\card{A}</script> is
the number of characters in our alphabet. <script type="math/tex">M_{ck}</script> is the probability of
observing character <script type="math/tex">c</script> at position <script type="math/tex">k</script> if <script type="math/tex">k \ge 1</script>, and <script type="math/tex">M_{c0}</script> is
the probability of observing character <script type="math/tex">c</script> in the background (i.e., the bases
of the sequence not included in the motif).</p>

<p>We define our data to be a collection of sequences <script type="math/tex">X = \{ X_1, \cdots, X_n
\}</script>.</p>

<h4 id="identifying-the-hidden-variables">Identifying the hidden variables</h4>

<p>We define our hidden states, the motif start positions, to be a matrix <script type="math/tex">Z</script>
such that <script type="math/tex">Z_{ij}</script> is 1 if the motif starts at position <script type="math/tex">j</script> in sequence
<script type="math/tex">i</script>, and 0 otherwise. (To simplify, for now we assume that all sequences are
the same length <script type="math/tex">L</script> and <script type="math/tex">Z</script> is <script type="math/tex">n \times L</script>.)</p>

<h4 id="defining-the-probability-of-an-observed-sequence-given-our-model">Defining the probability of an observed sequence given our model</h4>

<p>Given a motif start position, the probability of the observed sequence can now be calculated:</p>

<script type="math/tex; mode=display">P(X_i \mid Z_{ij} = 1, M) = \prod\limits_{k=1}^{j-1} M_{c_k, 0} \prod\limits_{k=j}^{j+W-1} M_{c_k, k-j+1} \prod\limits_{k=j+W}^{L} M_{c_k, 0}</script>

<p>The first product represents the probability of the characters in the sequence
before the motif starts, based on the background probabilities; the middle
product represents the probability of the putative motif sequence based on the
motif model; and the final product represents the probability of the characters
in the sequence after the motif ends, again based on the background
probabilities.</p>

<h4 id="defining-the-em-algorithm">Defining the EM algorithm</h4>

<p>EM takes the maximum likelihood across all sequences, so we need a likelihood
function. EM indirectly optimizes the log likelihood of the observed data,
<script type="math/tex">\log P(X \mid M)</script>, and the M-step requires a joint log likelihood</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\L &= \log P(X, Z \mid M) && \text{$X$ = all sequences} \\
   &= \log \prod\limits_i P(X_i, Z_i \mid M) && \text{Sequences are independent} \\
   &= \log \prod\limits_i P(X_i \mid Z_i, M) P(Z_i \mid M) && \text{Chain rule} 
\end{align*} %]]></script>

<p>Note that we have already defined <script type="math/tex">P(X_i \mid Z_i, M)</script> in our model above.
For the second term, we can assume that all positions within a sequence are
equally likely to contain the motif start position, and therefore define
<script type="math/tex">P(Z_i \mid M) = \frac{1}{L}</script>. Substituting in, we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\L &= \log \prod\limits_i \frac{1}{L} \prod\limits_j P(X_i \mid Z_{ij} = 1, M)^{Z_{ij}}
\end{align*} %]]></script>

<p>Here, <script type="math/tex">j</script> is denoting the possible indices in each sequence where the motif
could start. Recall that <script type="math/tex">Z_{ij}</script> is 1 when the motif starts at position
<script type="math/tex">j</script> in sequence <script type="math/tex">i</script>, so by raising this probability to the <script type="math/tex">Z_{ij}</script>, we
are keeping only the probabilities for the sites with a motif start (where
<script type="math/tex">Z_{ij}=1</script>).</p>

<p>A log of a product is equal to a sum of logs, and we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\L &= \sum\limits_i \sum\limits_j Z_{ij} \log P(X_i \mid Z_{ij} = 1, M) + n \log \frac{1}{L}
\end{align*} %]]></script>

<p>where we will return in the next class.</p>

  </div><a class="u-url" href="/notes/bmi776/lecture-02" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Matthew Stone</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Matthew Stone</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/msto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">msto</span></a></li><li><a href="https://www.twitter.com/m_sto"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">m_sto</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Under construction
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
